{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.),\n",
       " tensor(0.),\n",
       " tensor([[0., 1.],\n",
       "         [4., 9.]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "\n",
    "def corr2d(X,K):\n",
    "  h,w=K.shape\n",
    "  Y=torch.zeros(X.shape[0]-h+1,X.shape[1]-w+1)\n",
    "  for i in range(Y.shape[0]):\n",
    "    for j in range(Y.shape[1]):\n",
    "      Y[i,j]=(K*X[i:i+h,j:j+w]).sum()\n",
    "      \n",
    "  return Y\n",
    "\n",
    "\n",
    "corr2d(X, K)\n",
    "K[0][0],K[0,0],K[0:2][0:2]*K[0:2][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出的卷积层有时被称为特征映射（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。 在卷积神经网络中，对于某一层的任意元素 x ，其感受野（receptive field）是指在前向传播期间可能影响 x 计算的所有元素（来自所有先前层）。-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1=9.73404598236084\n",
      "loss3=3.261800765991211\n",
      "loss5=1.2143547534942627\n",
      "loss7=0.4769849479198456\n",
      "loss9=0.1919480264186859\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.9460, -1.0360]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "K=torch.tensor([[1.0,-1.0]])\n",
    "Y = corr2d(X, K)  \n",
    "lr=3e-2\n",
    "conv2d=nn.Conv2d(1,1,(1,2),bias=False)\n",
    "X=X.reshape((1,1,6,8))\n",
    "Y=Y.reshape((1,1,6,7))\n",
    "for epoch in range(10):\n",
    "  \n",
    "  Y_hat=conv2d(X)\n",
    "  l=(Y_hat-Y)**2\n",
    "  conv2d.zero_grad()\n",
    "  l.sum().backward()\n",
    "  conv2d.weight.data[:]-=lr*conv2d.weight.grad\n",
    "  if (epoch+1)%2==0:\n",
    "    print(f'loss{epoch}={l.sum()}')\n",
    "\n",
    "print(conv2d.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 padding and stride\n",
    "\n",
    "在实践中，我们很少使用不一致的步幅或填充，\n",
    "\n",
    "此外，使用奇数的核大小和填充大小也提供了书写上的便利。对于任何二维张量X，当满足： 1. 卷积核的大小是奇数； 2. 所有边的填充行数和列数相同； 3. 输出与输入具有相同高度和宽度 则可以得出：输出Y[i, j]是通过以输入X[i, j]为中心，与卷积核进行互相关计算得到的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8]) (1, 1, 8, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.randn(8,8)\n",
    "print(X.shape,(1,1)+X.shape)\n",
    "\n",
    "def show_result_shape(conv,X):\n",
    "  X=X.reshape((1,1)+X.shape)\n",
    "  Y=conv(X)\n",
    "  return Y.reshape(Y.shape[2:])\n",
    "\n",
    "#padding is set to ((h,w)-1)/2 to keep input,output same\n",
    "conv2d=nn.Conv2d(1,1,(3,3),padding=1)\n",
    "show_result_shape(conv2d,X).shape\n",
    "\n",
    "conv2d=nn.Conv2d(1,1,(5,3),padding=(2,1))\n",
    "show_result_shape(conv2d,X).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi input channel\n",
    "\n",
    "在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以将每个通道看作是对不同特征的响应。而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。因此，多输出通道并不仅是学习多个单通道的检测器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2]) tensor([[ 56.,  72.],\n",
      "        [104., 120.]])\n"
     ]
    }
   ],
   "source": [
    "def corr2d_multi_in(X,K):\n",
    "  #这里自动for第一维\n",
    "  return sum(corr2d(x,k) for x,k in zip(X,K))\n",
    "\n",
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "\n",
    "y=corr2d_multi_in(X, K)\n",
    "print(y.shape,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n",
      "torch.Size([3, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corr2d_multi_in_out(X,K):\n",
    "  return torch.stack([corr2d_multi_in(X,k) for k in K],0)\n",
    "\n",
    "#stack 操作：指定维度，堆叠\n",
    "print(K.shape)\n",
    "K=torch.stack((K,K+1,K+2),0)\n",
    "print(K.shape)\n",
    "corr2d_multi_in_out(X,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1*1卷积层\n",
    "\n",
    "可以将 1×1 卷积层看作是在每个像素位置应用的全连接层，以 ci 个输入值转换为 co 个输出值。\n",
    "\n",
    "当执行 1×1 卷积运算时，上述函数相当于先前实现的互相关函数corr2d_multi_in_out。（why？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "tensor([[[-2.5170,  2.4918, -0.3901],\n",
      "         [ 2.2365, -0.9521,  1.6288],\n",
      "         [ 0.9338, -1.9884,  1.4784]],\n",
      "\n",
      "        [[-0.7450,  0.2705, -0.4235],\n",
      "         [ 1.4650, -1.3130,  0.1807],\n",
      "         [-0.3206, -0.5276,  0.0146]]])\n"
     ]
    }
   ],
   "source": [
    "def corr2d_multi_in_out_1x1(X,K):\n",
    "  c_i,h,w=X.shape\n",
    "  c_o=K.shape[0]\n",
    "  X=X.reshape(c_i,-1)\n",
    "  K=K.reshape(c_o,-1)\n",
    "  Y=torch.matmul(K,X)\n",
    "  #@ for np, can't parellel\n",
    "  Y=K@X\n",
    "  return Y.reshape((c_o,h,w))\n",
    "\n",
    "X = torch.normal(0, 1, (3, 3, 3))\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6\n",
    "print(float(torch.abs(Y1 - Y2).sum()))\n",
    "print(Y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "疑问\n",
    "1.for 多维，只循环最外维吗？还是说回根据函数参数（有点不可能\n",
    "\n",
    "2.多维 矩阵乘法 还是不大懂唉，虽然好像先转成了2维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n",
      "tensor([[0, 1],\n",
      "        [2, 3]])\n",
      "tensor([[4, 5],\n",
      "        [6, 7]])\n"
     ]
    }
   ],
   "source": [
    "k=torch.arange(8)\n",
    "k=k.reshape(2,2,2)\n",
    "print(k.shape)\n",
    "for x in k:\n",
    "  print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pooling\n",
    "在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "\n",
    "pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15.]]]])\n",
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15.]],\n",
      "\n",
      "         [[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.],\n",
      "          [13., 14., 15., 16.]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "print(X)\n",
    "X=torch.cat((X,X+1),1)\n",
    "print(X)\n",
    "\n",
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "996a5e610904d34c377d92f469d8d0493e6171339ff9820ff1b3cb3ff50ffb76"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('d2l-22-1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
